---
title: "Prediction Model - Exercise Manner"
author: "Brandon Hu"
date: "March 20, 2016"
output: html_document
---

###Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

This project goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

More information is available from the website here: <http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset).

###Data Preparation
***
Download and load test and training data.

```{r}
# Data Loading
#-------------
# trainingDataUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
# testDataUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
# Loading the data
# download.file(trainingDataUrl,destfile = "pml_train.csv")
# download.file(testDataUrl,destfile = "pml_test.csv")

testingData = read.csv("pml-training.csv", na.strings=c("", "NA", "NULL"))
dim(testingData)

trainingData = read.csv("pml-training.csv", na.strings=c("", "NA", "NULL"))
dim(trainingData)
```

**Pre-screening Data**
There are several approaches for reducing the number of predictors.

1. Remove variables that we believe have too many NA values.
```{r}
training.dena <- trainingData[ , colSums(is.na(trainingData)) == 0]
dim(training.dena)
```

2. Remove unrelevant variables There are some unrelevant variables that can be removed as they are unlikely to be related to dependent variable.

```{r}
remove = c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window')
training.dere <- training.dena[, -which(names(training.dena) %in% remove)]
dim(training.dere)
```
3. Check the variables that have extremely low variance (this method is useful nearZeroVar() )

```{r}
#install caret using: install.packages('caret', repos='http://cran.rstudio.com/')
#suppress messages and warnings if any from loading 'caret' package
suppressWarnings(suppressMessages(library(caret)))

# only numeric variabls can be evaluated in this way.
zeroVar= nearZeroVar(training.dere[sapply(training.dere, is.numeric)], saveMetrics = TRUE)
training.nonzerovar = training.dere[,zeroVar[, 'nzv']==0]
dim(training.nonzerovar)
```
4. Remove highly correlated variables 90% (using for example findCorrelation() )
```{r, echo=TRUE}
# only numeric variabls can be evaluated in this way.
corrMatrix <- cor(na.omit(training.nonzerovar[sapply(training.nonzerovar, is.numeric)]))
dim(corrMatrix)
# there are 52 variables.
corrDF <- expand.grid(row = 1:52, col = 1:52)
corrDF$correlation <- as.vector(corrMatrix)
levelplot(correlation ~ row+ col, corrDF)
```
5. Lastly, remove those variable which have high correlation.

```{r, echo=TRUE}
removecor = findCorrelation(corrMatrix, cutoff = .90, verbose = FALSE)
training.decor = training.nonzerovar[,-removecor]
dim(training.decor)
```
As a result, left 19622 samples and 46 variables. Next split data to training and testing for cross validation for analysis purpose.

```{r}
inTrain <- createDataPartition(y=training.decor$classe, p=0.7, list=FALSE)
training <- training.decor[inTrain,]; testing <- training.decor[-inTrain,]
dim(training);
dim(testing)
```
###Analysis
***
Different prediction models will be used for analysis purpose.

**Regression Tree**

Fit a tree to these data, and summarize and plot it. First, use the 'tree' package as it is much faster than 'caret' package.

```{r}
suppressWarnings(suppressMessages(library(tree)))
set.seed(12345)
tree.training=tree(classe~.,data=training)
summary(tree.training)
plot(tree.training)
text(tree.training,pretty=0, cex =.8)
```
This is a bushy tree, next is to prune it.

**Rpart Model**
```{r}
#To prevent e1071 package error, install.packages('e1071', dependencies=TRUE)
modFit <- train(classe ~ .,method="rpart",data=training)
print(modFit$finalModel)
```
To generate prettier plots RPart decision tree plotter,
```{r}
#install.packages('rattle') from R and RTK2 from R
suppressWarnings(suppressMessages(library(rattle)))
fancyRpartPlot(modFit$finalModel)
```
The result from 'caret' 'rpart' package is close to 'tree' package. 

###Cross Validation
Perform cross validation to check the performance of the tree on the testing data.
```{r}
tree.pred=predict(tree.training,testing,type="class")
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix)) # error rate
```
The 0.70 is not very accurate.
```{r}
tree.pred=predict(modFit,testing)
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix)) # error rate
```
The 0.50 from 'caret' package is much lower than the result from 'tree' package.

###Pruning tree

This tree was grown to full depth, and might be too variable. Use Cross Validation to prune it.
```{r}
cv.training=cv.tree(tree.training,FUN=prune.misclass)
cv.training
plot(cv.training)
```
It shows that when the size of the tree goes down, the deviance goes up. It means the 21 is a good size (i.e. number of terminal nodes) for this tree so do not need to prune it.

Suppose we prune it at size of nodes at 18.
```{r}
prune.training=prune.misclass(tree.training,best=18)
```
Now lets evaluate this pruned tree on the test data.

```{r}
tree.pred=predict(prune.training,testing,type="class")
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix)) # error rate
```
0.66 is a little less than 0.70, so pruning did not affect much with repect to misclassification errors, and produce a simpler tree with less predictors to get almost the same result. By pruning, a shallower tree is generated and is easier to interpret.

The single tree is not good enough, so use bootstrap to improve the accuracy by using random forests.

**Random Forests**
Random forests build lots of bushy trees, and then average them to reduce the variance.
```{r}
require(randomForest)
set.seed(12345)
```
Lets fit a random forest and see how well it performs.
```{r}
rf.training=randomForest(classe~.,data=training,ntree=100, importance=TRUE)
rf.training
varImpPlot(rf.training)
```
As a result able to see which variables have higher impact on the prediction.

###Out-of Sample Accuracy
***
Random Forest model shows OOB estimate of error rate: 0.72% for the training data. Now we will predict it for out-of sample accuracy.

Now lets evaluate this tree on the test data.

```{r}
tree.pred=predict(rf.training,testing,type="class")
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix)) # error rate
```
0.99 means we got a very accurate estimate.

No. of variables tried at each split: 6. It means every time, only randomly use 6 predictors to grow the tree. Since p = 43, means can have it from 1 to 43, but it seems 6 is enough to get the good result.

###Conclusion
Since random forest gives a good prediction, it will be used to prediction model for this assignment.
```{r}
answers <- predict(rf.training, testingData)
answers
```